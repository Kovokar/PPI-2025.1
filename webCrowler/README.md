# ğŸ•·ï¸ Web Crawler Inteligente

## ğŸ“ DescriÃ§Ã£o do Projeto

Projeto de Web Crawler inteligente desenvolvido em **Node.js**, capaz de explorar e extrair informaÃ§Ãµes de pÃ¡ginas HTML de forma recursiva e eficiente.

---

## ğŸš€ Funcionalidades Principais

- Crawling recursivo de pÃ¡ginas web  
- ExtraÃ§Ã£o de links e conteÃºdo textual  
- Busca e contagem de ocorrÃªncias de termos  
- AnÃ¡lise de links e rastreamento de pÃ¡ginas visitadas  

---

## ğŸ› ï¸ Tecnologias Utilizadas

- Node.js  
- Axios (requisiÃ§Ãµes HTTP)  
- Cheerio (parser de HTML)  
- Path (manipulaÃ§Ã£o de caminhos)  

---

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Node.js (versÃ£o 14 ou superior)  
- npm (gerenciador de pacotes)  

### Passos

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/web-crawler.git
cd web-crawler

# Instale as dependÃªncias
npm install

```

## ğŸ”§ ConfiguraÃ§Ã£o

### ParÃ¢metros principais

- `BASE_URL`: URL base para crawling (exemplo: `http://127.0.0.1:5500/paginas/`)  
- `paginasIniciais`: Lista de pÃ¡ginas iniciais para iniciar o processo  

---

## ğŸ’» Uso BÃ¡sico

### Exemplo de cÃ³digo

```javascript
const { executarCrawler } = require('./back/services/webCrawler');

async function main() {
  // Executa o crawler
  const resultado = await executarCrawler();

  // Busca ocorrÃªncias de um termo
  const ocorrenciasDuna = resultado.buscarOcorrencias('duna');
  console.log(ocorrenciasDuna);
}

main();
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

As configuraÃ§Ãµes principais podem ser encontradas em `config.js` (ou no local onde vocÃª definiu as constantes):

```javascript
const BASE_URL = 'http://127.0.0.1:5500/paginas/';
const paginasIniciais = ['index.html'];

```
BASE_URL: Define a URL base para o processo de crawling. Certifique-se de que ela aponte para uma pasta local com pÃ¡ginas HTML.

paginasIniciais: Lista de pÃ¡ginas iniciais que o crawler deve visitar primeiro.

VocÃª pode personalizar essas variÃ¡veis para apontar para outras pÃ¡ginas locais ou estruturas diferentes.


---